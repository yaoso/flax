{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z1q_WRdTpgv"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/state_params.ipynb)\n",
    "[![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/state_params.ipynb)\n",
    "\n",
    "# 管理参数和状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KL1SWMpnUTAA",
    "scrolled": true,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/lj/.local/lib/python3.8/site-packages (22.1.2)\n",
      "Requirement already satisfied: jax in /home/lj/.local/lib/python3.8/site-packages (0.3.14)\n",
      "Requirement already satisfied: jaxlib in /home/lj/.local/lib/python3.8/site-packages (0.3.14)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/lj/.local/lib/python3.8/site-packages (from jax) (1.23.0)\n",
      "Requirement already satisfied: typing-extensions in /home/lj/.local/lib/python3.8/site-packages (from jax) (4.2.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/lj/.local/lib/python3.8/site-packages (from jax) (1.8.1)\n",
      "Requirement already satisfied: etils[epath] in /home/lj/.local/lib/python3.8/site-packages (from jax) (0.6.0)\n",
      "Requirement already satisfied: opt-einsum in /home/lj/.local/lib/python3.8/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: absl-py in /home/lj/.local/lib/python3.8/site-packages (from jax) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/lj/.local/lib/python3.8/site-packages (from jaxlib) (1.12)\n",
      "Requirement already satisfied: zipp in /home/lj/.local/lib/python3.8/site-packages (from etils[epath]->jax) (3.8.0)\n",
      "Requirement already satisfied: importlib_resources in /home/lj/.local/lib/python3.8/site-packages (from etils[epath]->jax) (5.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/google/flax.git\n",
      "  Cloning https://github.com/google/flax.git to /tmp/pip-req-build-553kt0ur\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google/flax.git /tmp/pip-req-build-553kt0ur\n",
      "  Resolved https://github.com/google/flax.git to commit e57133cee08fd979e32b10426b5ec444957b65dd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (1.23.0)\n",
      "Requirement already satisfied: jax>=0.3.2 in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (0.3.14)\n",
      "Requirement already satisfied: matplotlib in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (3.5.2)\n",
      "Requirement already satisfied: msgpack in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (1.0.4)\n",
      "Requirement already satisfied: optax in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (0.1.2)\n",
      "Requirement already satisfied: rich~=11.1 in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (11.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/lj/.local/lib/python3.8/site-packages (from flax==0.5.2) (4.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax==0.5.2) (5.4.1)\n",
      "Requirement already satisfied: etils[epath] in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.3.2->flax==0.5.2) (0.6.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.3.2->flax==0.5.2) (1.8.1)\n",
      "Requirement already satisfied: opt-einsum in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.3.2->flax==0.5.2) (3.3.0)\n",
      "Requirement already satisfied: absl-py in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.3.2->flax==0.5.2) (1.1.0)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/lib/python3/dist-packages (from rich~=11.1->flax==0.5.2) (0.4.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/lj/.local/lib/python3.8/site-packages (from rich~=11.1->flax==0.5.2) (2.12.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/lj/.local/lib/python3.8/site-packages (from rich~=11.1->flax==0.5.2) (0.9.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lj/.local/lib/python3.8/site-packages (from matplotlib->flax==0.5.2) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/lj/.local/lib/python3.8/site-packages (from matplotlib->flax==0.5.2) (9.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib->flax==0.5.2) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lj/.local/lib/python3.8/site-packages (from matplotlib->flax==0.5.2) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from matplotlib->flax==0.5.2) (20.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lj/.local/lib/python3.8/site-packages (from matplotlib->flax==0.5.2) (4.34.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lj/.local/lib/python3.8/site-packages (from matplotlib->flax==0.5.2) (2.8.2)\n",
      "Requirement already satisfied: chex>=0.0.4 in /home/lj/.local/lib/python3.8/site-packages (from optax->flax==0.5.2) (0.1.3)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /home/lj/.local/lib/python3.8/site-packages (from optax->flax==0.5.2) (0.3.14)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/lj/.local/lib/python3.8/site-packages (from chex>=0.0.4->optax->flax==0.5.2) (0.12.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /home/lj/.local/lib/python3.8/site-packages (from chex>=0.0.4->optax->flax==0.5.2) (0.1.7)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/lj/.local/lib/python3.8/site-packages (from jaxlib>=0.1.37->optax->flax==0.5.2) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->flax==0.5.2) (1.14.0)\n",
      "Requirement already satisfied: importlib_resources in /home/lj/.local/lib/python3.8/site-packages (from etils[epath]->jax>=0.3.2->flax==0.5.2) (5.8.0)\n",
      "Requirement already satisfied: zipp in /home/lj/.local/lib/python3.8/site-packages (from etils[epath]->jax>=0.3.2->flax==0.5.2) (3.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optax in /home/lj/.local/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /home/lj/.local/lib/python3.8/site-packages (from optax) (0.3.14)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/lj/.local/lib/python3.8/site-packages (from optax) (1.23.0)\n",
      "Requirement already satisfied: chex>=0.0.4 in /home/lj/.local/lib/python3.8/site-packages (from optax) (0.1.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /home/lj/.local/lib/python3.8/site-packages (from optax) (4.2.0)\n",
      "Requirement already satisfied: jax>=0.1.55 in /home/lj/.local/lib/python3.8/site-packages (from optax) (0.3.14)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /home/lj/.local/lib/python3.8/site-packages (from optax) (1.1.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /home/lj/.local/lib/python3.8/site-packages (from chex>=0.0.4->optax) (0.1.7)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/lj/.local/lib/python3.8/site-packages (from chex>=0.0.4->optax) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.1.55->optax) (3.3.0)\n",
      "Requirement already satisfied: etils[epath] in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.1.55->optax) (0.6.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/lj/.local/lib/python3.8/site-packages (from jax>=0.1.55->optax) (1.8.1)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/lj/.local/lib/python3.8/site-packages (from jaxlib>=0.1.37->optax) (1.12)\n",
      "Requirement already satisfied: importlib_resources in /home/lj/.local/lib/python3.8/site-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\n",
      "Requirement already satisfied: zipp in /home/lj/.local/lib/python3.8/site-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "! pip3 install --upgrade pip jax jaxlib\n",
    "! pip3 install --upgrade git+https://github.com/google/flax.git\n",
    "! pip3 install optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "import optax\n",
    "\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# Initialize random variables\n",
    "dummy_input = jnp.ones((32, 5))\n",
    "\n",
    "X = random.uniform(random.PRNGKey(0), (128, 5),  minval=0.0, maxval=1.0)\n",
    "noise = random.uniform(random.PRNGKey(0), (),  minval=0.0, maxval=0.1)\n",
    "X += noise\n",
    "\n",
    "W = random.uniform(random.PRNGKey(0), (5, 1),  minval=0.0, maxval=1.0)\n",
    "b = random.uniform(random.PRNGKey(0), (),  minval=0.0, maxval=1.0)\n",
    "\n",
    "Y = jnp.matmul(X, W) + b\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO6VTPVVT702"
   },
   "outputs": [],
   "source": [
    "class BiasAdderWithRunningMean(nn.Module):\n",
    "  momentum: float = 0.9\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    is_initialized = self.has_variable('batch_stats', 'mean')\n",
    "    mean = self.variable('batch_stats', 'mean', jnp.zeros, x.shape[1:])  # variable()???\n",
    "    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
    "    if is_initialized:\n",
    "      mean.value = (self.momentum * mean.value +\n",
    "                    (1.0 - self.momentum) * jnp.mean(x, axis=0, keepdims=True))\n",
    "    return mean.value + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ3TYyIaTy8f"
   },
   "source": [
    "This example model is a minimal example that contains both parameters (declared with `self.param`) and state variables (declared with `self.variable`).\n",
    "\n",
    "The tricky part with initialization here is that we need to split the state variables and the parameters we’re going to optimize for.\n",
    "\n",
    "First we define `update_step` as follow (with dummy loss that should be replaced for yours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7xxRU02VeJb"
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def update_step(apply_fn, x, opt_state, params, state):\n",
    "  def loss(params):\n",
    "    y, updated_state = apply_fn({'params': params, **state},\n",
    "                                x, mutable=list(state.keys()))\n",
    "    l = ((x - y) ** 2).sum() # Replace with your loss here.\n",
    "    return l, updated_state\n",
    "\n",
    "  (l, updated_state), grads = jax.value_and_grad(\n",
    "      loss, has_aux=True)(params)\n",
    "  updates, opt_state = tx.update(grads, opt_state)  # Defined below.\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return opt_state, params, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM_EyHqwVlEw"
   },
   "source": [
    "Then we can write the actual training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RUFi57GVktj"
   },
   "outputs": [],
   "source": [
    "model = BiasAdderWithRunningMean()\n",
    "variables = model.init(random.PRNGKey(0), dummy_input)\n",
    "# Split state and params (which are updated by optimizer).\n",
    "state, params = variables.pop('params')\n",
    "del variables  # Delete variables to avoid wasting resources\n",
    "tx = optax.sgd(learning_rate=0.02)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "  opt_state, params, state = update_step(\n",
    "      model.apply, dummy_input, opt_state, params, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mKoPjtQVqD3"
   },
   "source": [
    "## vmap accross the batch dimension\n",
    "\n",
    "When using `vmap` and managing state that depends on the batch dimension, for example when using `BatchNorm`, the setup above must be modified slightly. This is because any layer whose state depends on the batch dimension is not strictly vectorizable. In the case of `BatchNorm`, `lax.pmean()` must be used to average the statistics over the batch dimension so that the state is in sync for each item in the batch.\n",
    "\n",
    "This requires two small changes. Firstly, we need to name the batch axis in our model definition. Here, this is done by specifying the `axis_name` argument of `BatchNorm`. In your own code this might require specifying the `axis_name` argument of `lax.pmean()` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r18oI6NOVp7Y"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  hidden_size: int\n",
    "  out_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, train=False):\n",
    "    norm = partial(\n",
    "        nn.BatchNorm,\n",
    "        use_running_average=not train,\n",
    "        momentum=0.9,\n",
    "        epsilon=1e-5,\n",
    "        axis_name=\"batch\", # Name batch dim\n",
    "    )\n",
    "\n",
    "    x = nn.Dense(self.hidden_size)(x)\n",
    "    x = norm()(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(self.hidden_size)(x)\n",
    "    x = norm()(x)\n",
    "    x = nn.relu(x)\n",
    "    y = nn.Dense(self.out_size)(x)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_3cI-2DWL3g"
   },
   "source": [
    "Secondly, we need to specify the same name when calling `vmap` in our training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jurc8PdWLVP"
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def update_step(apply_fn, x_batch, y_batch, opt_state, params, state):\n",
    "\n",
    "  def batch_loss(params):\n",
    "    def loss_fn(x, y):\n",
    "      pred, updated_state = apply_fn(\n",
    "        {'params': params, **state},\n",
    "        x, mutable=list(state.keys())\n",
    "      )\n",
    "      return (pred - y) ** 2, updated_state\n",
    "\n",
    "    loss, updated_state = jax.vmap(\n",
    "      loss_fn, out_axes=(0, None),  # Do not vmap `updated_state`.\n",
    "      axis_name='batch'  # Name batch dim\n",
    "    )(x_batch, y_batch)  # vmap only `x`, `y`, but not `state`.\n",
    "    return jnp.mean(loss), updated_state\n",
    "\n",
    "  (loss, updated_state), grads = jax.value_and_grad(\n",
    "    batch_loss, has_aux=True\n",
    "  )(params)\n",
    "\n",
    "  updates, opt_state = tx.update(grads, opt_state)  # Defined below.\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return opt_state, params, updated_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CI-cToTYURG"
   },
   "source": [
    "Note that we also need to specify that the model state does not have a batch dimension. Now we are able to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdMTQcMoYUtk"
   },
   "outputs": [],
   "source": [
    "model = MLP(hidden_size=10, out_size=1)\n",
    "variables = model.init(random.PRNGKey(0), dummy_input)\n",
    "# Split state and params (which are updated by optimizer).\n",
    "state, params = variables.pop('params')\n",
    "del variables  # Delete variables to avoid wasting resources\n",
    "tx = optax.sgd(learning_rate=0.02)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "  opt_state, params, state, loss = update_step(\n",
    "      model.apply, X, Y, opt_state, params, state)\n",
    "  print(f\"Loss for epoch {epoch_num + 1}:\", loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "state_params_howto.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
