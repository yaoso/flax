{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwtfSYdoHsc_"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/jax_for_the_impatient.ipynb)\n",
    "[![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/jax_for_the_impatient.ipynb)\n",
    "\n",
    "# JAX for the Impatient\n",
    "**JAX 等于 NumPy on the CPU, GPU, and TPU, 并且带有强大的自动微分能力，用于高性能机器学习研究。**\n",
    "\n",
    "这里我们讲一下JAX的基础，帮助你学习Flax，在熟悉了这些基础后，我们还是建议你去浏览JAX的文档 [here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF2oOT78zOIr"
   },
   "source": [
    "## NumPy API\n",
    "\n",
    "JAX中有一套NumPy API，先来看一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5csM8DZYEqk6",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp, random\n",
    "\n",
    "import numpy as np # We import the standard NumPy library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5BLL6v_JUSI"
   },
   "source": [
    "`jax.numpy` 是一套模拟NumPy的API，由于JAX的随机数生成机制和NumPy完全不同，我们还需要用到 `jax.random` 来生成一些随机数。\n",
    "\n",
    "我们来一个矩阵乘法的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2HKiLTNJ4Eh",
    "outputId": "c4297a1a-4e4b-4bdc-ca5d-3d33aca92b3b",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1., 1., 1., 1.],\n",
       "             [1., 1., 1., 1.],\n",
       "             [1., 1., 1., 1.],\n",
       "             [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = jnp.ones((4,4)) # 生成一个4 * 4 的矩阵\n",
    "n = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
    "               [5.0, 6.0, 7.0, 8.0]]) # 2 * 4的矩阵\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKFtn4d_Nu07"
   },
   "source": [
    "JAX中的数组类型是DeviceArray，我们可以和NumPy那样进行矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9do-ZRGaRThn",
    "outputId": "9c4feb4d-3bd1-4921-97ce-c8087b37496f",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[10., 10., 10., 10.],\n",
       "             [26., 26., 26., 26.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.dot(n, m).block_until_ready() # Note: yields the same result as np.dot(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jkyt5xXpRidn"
   },
   "source": [
    "JAX中默认异步执行，所以DeviceArray对象实际上是futures ([more here](https://jax.readthedocs.io/en/latest/async_dispatch.html)) 。 可能矩阵乘法还没计算完成，Python调用已经结束了，所以我们增加了 `block_until_ready()` 来保证Python程序返回最终的计算结果。\n",
    "\n",
    "JAX的DeviceArray和Numpy NDArray之间可以无缝转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFthGlHoRZ59",
    "outputId": "15892d6a-c06c-4f98-a7d4-ad432bdd1f57",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2.       ,  2.       ,  2.       ,  2.       ],\n",
       "             [ 1.7832031,  1.7832031,  1.7832031,  1.7832031],\n",
       "             [-1.3183594, -1.3183594, -1.3183594, -1.3183594],\n",
       "             [ 1.9140625,  1.9140625,  1.9140625,  1.9140625]],            dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(size=(4,4)) # 创建一个NumPy数组\n",
    "jnp.dot(x, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoaA-FS2XpsC"
   },
   "source": [
    "如果你在GPU或TPU上运行JAX，使用NumPy数组可能会产生多次复制操作，将数组从CPU复制到GPU/TPU。建议使用JAX数组或者调用 `jax.device_put` 将NumPy数组迁移到加速卡。JAX数组（DeviceArrays）就是在device上进行计算，不涉及到数据迁移，比如`jnp.dot(long_vector, long_vector)` 只会讲最后的结果（标量）从device迁移到host。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VABtdIwTFfN",
    "outputId": "08965869-bdd7-44c8-ae46-207061b5112c",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.9078977 , -0.14710547, -0.72077036, -0.9940185 ],\n",
       "             [ 0.86262065, -1.0833409 ,  0.13059273, -0.5004832 ],\n",
       "             [-1.2784858 ,  1.0578346 , -0.71898067,  1.2214077 ],\n",
       "             [ 0.04497718,  1.4795924 , -0.17639156, -1.4458165 ]],            dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(size=(4,4))\n",
    "x = jax.device_put(x)  # device_put()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_2QavY1tR8j"
   },
   "source": [
    "怎杨将JAX数组转换为NumPy数组？so easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEJ1mSvStjEC",
    "outputId": "00a8cc38-59a2-4cf9-ed23-eb5fbb708495",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4.],\n",
       "       [5., 6., 7., 8.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
    "               [5.0, 6.0, 7.0, 8.0]])\n",
    "np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBHVd3GTpLKD"
   },
   "source": [
    "## 不可修改（Immutability）\n",
    "\n",
    "JAX本质上是函数式灵魂，导致JAX数组是不可变的，不可能对JAX数组进行原地（in-place）赋值或者切片赋值，并且，函数不应该读写全局状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-erZrgZXawFW",
    "outputId": "c3c03081-6235-482f-a88c-cc180f661954",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " [[1. 2. 3. 4.]\n",
      " [5. 6. 7. 8.]]\n",
      "updated: \n",
      " [[3. 2. 3. 4.]\n",
      " [5. 6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([[1.0, 2.0, 3.0, 4.0],\n",
    "               [5.0, 6.0, 7.0, 8.0]])\n",
    "updated = x.at[0, 0].set(3.0) # 如果执行 x[0,0] = 3.0 会报错\n",
    "print(\"x: \\n\", x) # 注意x并没有被修改\n",
    "print(\"updated: \\n\", updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz_9b-XUTjjl"
   },
   "source": [
    "`at[]`除了支持`set()`，还支持 `add`, `mul`, `min`, `max`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8QGdusyzbmP"
   },
   "source": [
    "## 随机数\n",
    "\n",
    "JAX和NumPy的一大区别就是，随机数。详情建议阅读 JAX文档 [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers) ，这里我们直接转述了:\n",
    "\n",
    "*JAX中的伪随机数生成算法和NumPy不同，最主要的是，JAX不会隐式的修改随机数状态，必须用户显式修改。*\n",
    "\n",
    "\n",
    "由于采用了更现代的随机数生成算法，JAX中的随机数状态很简单，就是一个包含两个unsigned-int32s的向量，这个向量被称为key。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iz9KGF4s7nN",
    "outputId": "c5bb1581-090b-42ed-cc42-08436154bc14",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 0], dtype=uint32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y622foIaYjL"
   },
   "source": [
    "如果你用相同的key来生成随机数，也就是等价于用用相同的随机数状态来生成随机数，得到的结果是什么？同一个随机数生成函数得到的随机数一直不变！你必须手动修改随机数状态，也就是修改key，怎么改呢？很简单，split就行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the random number using key:  [0 0]  gives:  [-0.20584236]\n",
      "Printing the random number using key:  [0 0]  gives:  [-0.20584236]\n",
      "Printing the random number using key:  [0 0]  gives:  [-0.20584236]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Printing the random number using key: \", key, \" gives: \", random.normal(key,shape=(1,))) # 同一个key，同一个随机数生成算法，结果相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOBv5CaB3dMa",
    "outputId": "ac89afdc-a73e-4c31-d005-7e1e6ad551cd",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old key [0 0] --> normal [-0.20584236]\n",
      "    \\---SPLIT --> new key    [4146024105  967050713] --> normal [0.14389051]\n",
      "             \\--> new subkey [2718843009 1272950319] --> normal [-1.2515285]\n"
     ]
    }
   ],
   "source": [
    "print(\"old key\", key, \"--> normal\", random.normal(key, shape=(1,)))\n",
    "key, subkey = random.split(key)  # 创建新的key\n",
    "print(\"    \\---SPLIT --> new key   \", key, \"--> normal\", random.normal(key, shape=(1,)) )\n",
    "print(\"             \\--> new subkey\", subkey, \"--> normal\", random.normal(subkey, shape=(1,)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgCCZtyQ4EqA"
   },
   "source": [
    "split也可以得到多个subkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3zRojMs4Cce",
    "outputId": "e48e1ed0-4f16-49cb-dc2b-cb51d3ec56b5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([3306097435, 3899823266], dtype=uint32),\n",
       " [DeviceArray([147607341, 367236428], dtype=uint32),\n",
       "  DeviceArray([2280136339, 1907318301], dtype=uint32),\n",
       "  DeviceArray([ 781391491, 1939998335], dtype=uint32)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, *subkeys = random.split(key, 4)\n",
    "key, subkeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC6-1gq1YsgZ"
   },
   "source": [
    "## 梯度和自动微分（autodiff）\n",
    "\n",
    "如果向全面了解JAX的自动微分机制，还是看JAX文档 [Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).\n",
    "\n",
    "Even though, theoretically, a VJP (Vector-Jacobian product - reverse autodiff) and a JVP (Jacobian-Vector product - forward-mode autodiff) are similar—they compute a product of a Jacobian and a vector—they differ by the computational complexity of the operation. In short, when you have a large number of parameters (hence a wide matrix), a JVP is less efficient computationally than a VJP, and, conversely, a JVP is more efficient when the Jacobian matrix is a tall matrix. You can read more in the JAX [cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobian-vector-products-jvps-aka-forward-mode-autodiff) [notebook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#vector-jacobian-products-vjps-aka-reverse-mode-autodiff) mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUFwVnn4011l"
   },
   "source": [
    "### 梯度\n",
    "\n",
    "JAX提供了强大的自动微分功能，这也是函数式编程的优势，由于对函数求导本质上是无状态的。 \n",
    "\n",
    "考虑后一个简单的函数 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$\n",
    "\n",
    "$$f(x) = \\frac{1}{2} x^T x$$\n",
    "\n",
    "它的导函数是:\n",
    "$$\\nabla f(x) = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDOydrLMcIzp",
    "outputId": "580c14ed-d1a3-4f92-c9b9-78d58c87bc76",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "def f(x):\n",
    "  return jnp.dot(x.T,x)/2.0\n",
    "\n",
    "v = jnp.ones((4,))\n",
    "f(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVaiZplShoBK"
   },
   "source": [
    "JAX计算导函数是如此简单， `jax.grad`，注意只能作用于输出标量的函数。\n",
    "\n",
    "下面对f求导，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ael3pVHmhhTs",
    "outputId": "4d0c5122-1ead-4a94-9153-7eb3b399dae2",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original v:\n",
      "[ 1.8160864  -0.75487745  0.3398885  -0.53483075]\n",
      "Gradient of f taken at point v\n",
      "[ 1.8160864  -0.75487745  0.3398885  -0.53483075]\n"
     ]
    }
   ],
   "source": [
    "v = random.normal(key,(4,))\n",
    "print(\"Original v:\")\n",
    "print(v)\n",
    "print(\"Gradient of f taken at point v\")\n",
    "print(jax.grad(f)(v)) # should be equal to v !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHIMfchIiQMR"
   },
   "source": [
    "前面讲过， `jax.grad` 只能用于返回标量的函数，虽然对深度学习已经足够了，但是JAX也支持通用的向量函数，可以使用功能更强大的原语 Jacobian-Vector product - `jax.jvp` - 和 Vector-Jacobian product - `jax.vjp`。\n",
    "\n",
    "### Jacobian-Vector product\n",
    "\n",
    "Let's consider a map $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$. As a reminder, the differential of f is the map $df:\\mathbb{R}^n \\rightarrow \\mathcal{L}(\\mathbb{R}^n,\\mathbb{R}^m)$ where $\\mathcal{L}(\\mathbb{R}^n,\\mathbb{R}^m)$ is the space of linear maps from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ (hence $df(x)$ is often represented as a Jacobian matrix). The linear approximation of f at point $x$ reads:\n",
    "$$f(x+v) = f(x) + df(x)\\bullet v + o(v)$$\n",
    "\n",
    "The $\\bullet$ operator means you are applying the linear map $df(x)$ to the vector v.\n",
    "\n",
    "Even though you are rarely interested in computing the full Jacobian matrix representing the linear map $df(x)$ in a standard basis, you are often interested in the quantity $df(x)\\bullet v$. This is exactly what `jax.jvp` is for, and `jax.jvp(f, (x,), (v,))` returns the tuple:\n",
    "$$(f(x), df(x)\\bullet v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5nI_gbeqj2y"
   },
   "source": [
    "Let's use a simple function as an example: $f(x) = \\frac{1}{2}({x_1}^2, {x_2}^2, \\ldots, {x_n}^2)$ where we know that $df(x)\\bullet h = (x_1h_1, x_2h_2,\\ldots,x_nh_n)$. Hence using `jax.jvp` with $h= (1,1,\\ldots,1)$ should return $x$ as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2ntaHBeh-5u",
    "outputId": "93591ad3-832f-4928-c1f8-073cc3b7aae7",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x,f(x))\n",
      "(DeviceArray([ 0.18784378, -1.2833427 , -0.27109176,  1.2490592 ,\n",
      "              0.24446994], dtype=float32), DeviceArray([0.01764264, 0.82348424, 0.03674537, 0.7800744 , 0.02988278],            dtype=float32))\n",
      "jax.jvp(f, (x,),(v,))\n",
      "(DeviceArray([0.01764264, 0.82348424, 0.03674537, 0.7800744 , 0.02988278],            dtype=float32), DeviceArray([ 0.18784378, -1.2833427 , -0.27109176,  1.2490592 ,\n",
      "              0.24446994], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  return jnp.multiply(x,x)/2.0\n",
    "\n",
    "x = random.normal(key, (5,))\n",
    "v = jnp.ones(5)\n",
    "print(\"(x,f(x))\")\n",
    "print((x,f(x)))\n",
    "print(\"jax.jvp(f, (x,),(v,))\")\n",
    "print(jax.jvp(f, (x,),(v,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdm_TTDLal_X"
   },
   "source": [
    "### Vector-Jacobian product\n",
    "Keeping our $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ it's often the case (for example, when you are working with a scalar loss function) that you are interested in the composition $x\\rightarrow\\phi\\circ f(x)$ where $\\phi :\\mathbb{R}^m\\rightarrow\\mathbb{R}$. In that case, the gradient reads:\n",
    "$$\\nabla(\\phi\\circ f)(x) = J_f(x)^T\\nabla\\phi(f(x))$$\n",
    "\n",
    "Where $J_f(x)$ is the Jacobian matrix of f evaluated at x, meaning that $df(x)\\bullet v = J_f(x)v$.\n",
    "\n",
    "`jax.vjp(f,x)` returns the tuple:\n",
    "$$(f(x),v\\rightarrow v^TJ_f(x))$$\n",
    "\n",
    "Keeping the same example as previously, using $v=(1,\\ldots,1)$, applying the VJP function returned by JAX should return the $x$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1VTl9zXqsFl",
    "outputId": "f3f143a9-b1f1-4a4d-e4b1-c24a0fa114b8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [ 0.18784378 -1.2833427  -0.27109176  1.2490592   0.24446994]\n",
      "v^T Jf(x) =  [ 0.18784378 -1.2833427  -0.27109176  1.2490592   0.24446994]\n"
     ]
    }
   ],
   "source": [
    "(val, jvp_fun) = jax.vjp(f,x)\n",
    "print(\"x = \", x)\n",
    "print(\"v^T Jf(x) = \", jvp_fun(jnp.ones((5,)))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v1Uq_XlzRZS"
   },
   "source": [
    "## 使用jit和vmap加速代码执行\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF04t9L71dhH"
   },
   "source": [
    "### Jit\n",
    "\n",
    "JAX底层用的是XLA编译器，也支持用户手动使用JIT编译来进一步加速，`@jit`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6p_wQ9xeIiu",
    "outputId": "af7ea5af-5ee1-4aa5-d8d7-8f6a20da2b0e",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 ms ± 612 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "v = random.normal(key, (1000000,))\n",
    "%timeit selu(v).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk9LVX580j6M"
   },
   "source": [
    "使用JIT编译:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "us5pWySG0jWL",
    "outputId": "e8ff3b7b-3917-40fc-8f29-eb9e6df262e5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 µs ± 13.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jax.jit(selu)\n",
    "%timeit selu_jit(v).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kQyCgo407oF"
   },
   "source": [
    "---\n",
    "### 向量化（Vectorization）\n",
    "\n",
    "JAX允许你写的函数只作用于一个样本，由JAX来自动进行批处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-E6MsKF0tmZ",
    "outputId": "bfa377e8-92ee-4473-abd4-8d52338e2cc5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single apply shape:  (15,)\n",
      "Batched example shape:  (5, 15)\n"
     ]
    }
   ],
   "source": [
    "mat = random.normal(key, (15, 10))\n",
    "batched_x = random.normal(key, (5, 10)) # Batch size在维度0\n",
    "single = random.normal(key, (10,))\n",
    "\n",
    "def apply_matrix(v):\n",
    "  return jnp.dot(mat, v)\n",
    "\n",
    "print(\"Single apply shape: \", apply_matrix(single).shape)\n",
    "print(\"Batched example shape: \", jax.vmap(apply_matrix)(batched_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2BcA8wm2_FW"
   },
   "source": [
    "## 线性回归例子\n",
    "\n",
    "让我们实现一个线性回归的例子，训练集 $\\{(x_i,y_i), i\\in \\{1,\\ldots, k\\}, x_i\\in\\mathbb{R}^n,y_i\\in\\mathbb{R}^m\\}$，我们想找到一组最优的参数 $W\\in \\mathcal{M}_{m,n}(\\mathbb{R}), b\\in\\mathbb{R}^m$ 来让预测结果 $f_{W,b}(x)=Wx+b$ 和真实标签之间的MSE最小:\n",
    "$$\\mathcal{L}(W,b)\\rightarrow\\frac{1}{k}\\sum_{i=1}^{k} \\frac{1}{2}\\|y_i-f_{W,b}(x_i)\\|^2_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5W9p_zVe2Cj-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Linear feed-forward.\n",
    "def predict(W, b, x):\n",
    "  return jnp.dot(x, W) + b\n",
    "\n",
    "# Loss function: Mean squared error.\n",
    "def mse(W, b, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    y_pred = predict(W, b, x)\n",
    "    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n",
    "  # We vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qMkIxjjsduPY",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = predict(W, b, x_samples) + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L2np6wve_xp",
    "outputId": "9db5c834-d7da-4291-d1ec-d4c39008d5ed",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.02363973\n",
      "Loss step 0:  10.971258\n",
      "Loss step 5:  1.0783367\n",
      "Loss step 10:  0.37938032\n",
      "Loss step 15:  0.17768295\n",
      "Loss step 20:  0.09448608\n",
      "Loss step 25:  0.05415666\n",
      "Loss step 30:  0.034192722\n",
      "Loss step 35:  0.023958156\n",
      "Loss step 40:  0.018536853\n",
      "Loss step 45:  0.015428396\n",
      "Loss step 50:  0.01385448\n",
      "Loss step 55:  0.012978616\n",
      "Loss step 60:  0.012341755\n",
      "Loss step 65:  0.01210101\n",
      "Loss step 70:  0.011895995\n",
      "Loss step 75:  0.011840537\n",
      "Loss step 80:  0.011756034\n",
      "Loss step 85:  0.011716946\n",
      "Loss step 90:  0.011748294\n",
      "Loss step 95:  0.01174049\n",
      "Loss step 100:  0.011691327\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimated W and b with zeros.\n",
    "W_hat = jnp.zeros_like(W)\n",
    "b_hat = jnp.zeros_like(b)\n",
    "\n",
    "# Ensure we jit the largest-possible jittable block.\n",
    "@jax.jit\n",
    "def update_params(W, b, x, y, lr):\n",
    "  W, b = W - lr * jax.grad(mse, 0)(W, b, x, y), b - lr * jax.grad(mse, 1)(W, b, x, y)\n",
    "  return W, b\n",
    "\n",
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(W, b, x_samples, y_samples))\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  W_hat, b_hat = update_params(W_hat, b_hat, x_samples, y_samples, learning_rate)\n",
    "  if (i % 5 == 0):\n",
    "    print(f\"Loss step {i}: \", mse(W_hat, b_hat, x_samples, y_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJGKunxNzrxa"
   },
   "source": [
    "This is obviously an approximate solution to the linear regression problem (solving it would require a bit more work!), but here you have all the tools you would need if you wanted to do it the proper way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQXmL86aUS9x"
   },
   "source": [
    "## Refining a bit with pytrees\n",
    "\n",
    "Here we're going to elaborate on our previous example using JAX pytree data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZMUvyCgUzby"
   },
   "source": [
    "### Pytrees basics\n",
    "\n",
    "JAX中到处都有pytree的身影，Flax也是，更多内容建议看 [pytree page](https://jax.readthedocs.io/en/latest/pytrees.html) :\n",
    "\n",
    "*In JAX, a pytree is a container of leaf elements and/or more pytrees. Containers include lists, tuples, and dicts (JAX can be extended to consider other container types as pytrees, see Extending pytrees below). A leaf element is anything that’s not a pytree, e.g. an array. In other words, a pytree is just a possibly-nested standard or user-registered Python container. If nested, note that the container types do not need to match. A single “leaf”, i.e. a non-container object, is also considered a pytree.*\n",
    "\n",
    "```python\n",
    "[1, \"a\", object()] # 3 leaves: 1, \"a\" and object()\n",
    "\n",
    "(1, (2, 3), ()) # 3 leaves: 1, 2 and 3\n",
    "\n",
    "[1, {\"k1\": 2, \"k2\": (3, 4)}, 5] # 5 leaves: 1, 2, 3, 4, 5\n",
    "```\n",
    "\n",
    "JAX provides a few utilities to work with pytrees that live in the `tree_util` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9SNY5eA1UdkJ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "t = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LujWjwVQUeea"
   },
   "source": [
    "You will often come across `tree_map` function that maps a function f to a tree and its leaves. We used it in the previous section to display the shapes of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szDhssVBUjTa",
    "outputId": "9ae4ebf1-a3c4-4ecb-b3df-67c8450310f8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, {'k1': 4, 'k2': (9, 16)}, 25]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_map(lambda x: x*x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s167WGKUlZ9"
   },
   "source": [
    "Instead of applying a standalone function to each of the tree leaves, you can also provide a tuple of additional trees with similar shape to the input tree that will provide per leaf arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNOYK_E7UnOh",
    "outputId": "d211bf85-5993-488c-9fec-aeaf375df007",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, {'k1': 6, 'k2': (12, 20)}, 30]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tree_util.tree_map(lambda x: x*x, t)\n",
    "tree_util.tree_map(lambda x,y: x+y, t, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnE75pvlVDO5"
   },
   "source": [
    "### Linear regression with Pytrees\n",
    "\n",
    "Whereas our previous example was perfectly fine, we can see that when things get more complicated (as they will with neural networks), it will be harder to manage parameters of the models as we did.\n",
    "\n",
    "Here we show an alternative based on pytrees, using the same data from the previous example.\n",
    "Now, our `params` is a pytree containing both the `W` and `b` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8v8gNkvUVZnl",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Linear feed-forward that takes a params pytree.\n",
    "def predict_pytree(params, x):\n",
    "  return jnp.dot(x, params['W']) + params['b']\n",
    "\n",
    "# Loss function: Mean squared error.\n",
    "def mse_pytree(params, x_batched,y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x,y):\n",
    "    y_pred = predict_pytree(params, x)\n",
    "    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n",
    "  # We vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n",
    "\n",
    "# Initialize estimated W and b with zeros. Store in a pytree.\n",
    "params = {'W': jnp.zeros_like(W), 'b': jnp.zeros_like(b)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKP0X8rnWAiA"
   },
   "source": [
    "The great thing is that JAX is able to handle differentiation with respect to pytree parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zc7cMaiWSny",
    "outputId": "a69605cb-1eed-4f81-fc2e-93646c9694dd",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': DeviceArray([[-1.9287349e+00,  4.2963755e-01,  7.1613449e-01,\n",
       "                2.1056123e+00,  5.0405121e-01, -2.4983375e+00,\n",
       "               -6.3854176e-01, -2.2620213e+00, -1.3365206e+00,\n",
       "               -2.0426039e-01],\n",
       "              [ 1.1999468e+00, -9.4563609e-01, -1.0878400e+00,\n",
       "               -7.0340711e-01,  3.3224609e-01,  1.7538791e+00,\n",
       "               -7.1916544e-01,  1.0927428e+00, -1.4491037e+00,\n",
       "                5.9715635e-01],\n",
       "              [-1.4826509e+00, -7.6116532e-01,  2.2319858e-01,\n",
       "               -3.0391946e-01,  3.0397055e+00, -3.8419428e-01,\n",
       "               -1.8290073e+00, -2.3353369e+00, -1.1087127e+00,\n",
       "               -7.7453995e-01],\n",
       "              [ 8.2374442e-01, -9.9650609e-01, -7.6030111e-01,\n",
       "                6.3919222e-01, -6.0864899e-02, -1.0859716e+00,\n",
       "                1.2923398e+00, -4.9342898e-01, -1.4711156e-03,\n",
       "                1.2977618e+00],\n",
       "              [-4.5656446e-01, -1.3063025e-01, -3.9179009e-01,\n",
       "                2.1743817e+00, -5.3948693e-02,  4.5653123e-01,\n",
       "               -8.5279423e-01,  1.1709594e+00,  9.6438813e-01,\n",
       "               -2.3813749e-02]], dtype=float32),\n",
       " 'b': DeviceArray([ 1.0923628,  1.3121076, -2.9304824, -0.6492362,  1.1531248],            dtype=float32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(mse_pytree)(params, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW1IKnjqXFdN"
   },
   "source": [
    "Now using our tree of params, we can write the gradient descent in a simpler way using `jax.tree_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEntdcDBXBCj",
    "outputId": "f309aff7-2aad-453f-ad88-019d967d4289",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639774\n",
      "Loss step 0:  11.096583\n",
      "Loss step 5:  1.1743388\n",
      "Loss step 10:  0.32879353\n",
      "Loss step 15:  0.1398177\n",
      "Loss step 20:  0.07359565\n",
      "Loss step 25:  0.04415301\n",
      "Loss step 30:  0.029408678\n",
      "Loss step 35:  0.021554656\n",
      "Loss step 40:  0.017227933\n",
      "Loss step 45:  0.014798875\n",
      "Loss step 50:  0.013420242\n",
      "Loss step 55:  0.0126327025\n",
      "Loss step 60:  0.0121810865\n",
      "Loss step 65:  0.011921468\n",
      "Loss step 70:  0.011771992\n",
      "Loss step 75:  0.011685831\n",
      "Loss step 80:  0.011636148\n",
      "Loss step 85:  0.011607475\n",
      "Loss step 90:  0.011590928\n",
      "Loss step 95:  0.011581394\n",
      "Loss step 100:  0.011575883\n"
     ]
    }
   ],
   "source": [
    "# Always remember to jit!\n",
    "@jax.jit\n",
    "def update_params_pytree(params, learning_rate, x_samples, y_samples):\n",
    "  params = jax.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params,\n",
    "        jax.grad(mse_pytree)(params, x_samples, y_samples))\n",
    "  return params\n",
    "\n",
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse_pytree({'W': W, 'b': b}, x_samples, y_samples))\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  params = update_params_pytree(params, learning_rate, x_samples, y_samples)\n",
    "  if (i % 5 == 0):\n",
    "    print(f\"Loss step {i}: \", mse_pytree(params, x_samples, y_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `jax.grad()`, another useful function is `jax.value_and_grad()`, which returns the value of the input function and of its gradient.\n",
    "\n",
    "To switch from `jax.grad()` to `jax.value_and_grad()`, replace the training loop above with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Using jax.value_and_grad instead:\n",
    "loss_grad_fn = jax.value_and_grad(mse_pytree)\n",
    "for i in range(101):\n",
    "  # Note that here the loss is computed before the param update.\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = jax.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads)\n",
    "    if (i % 5 == 0):\n",
    "        print(f\"Loss step {i}: \", loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh-oo8jFUPNQ"
   },
   "source": [
    "That's all you needed to know to get started with Flax! To dive deeper, we very much recommend checking the JAX [docs](https://jax.readthedocs.io/en/latest/index.html)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "JAX for the impatient.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
